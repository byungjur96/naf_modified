exp:
  expdir: ./logs/

network:
  net_type: mlp # WIRE
  num_layers: 6
  hidden_dim: 128 # 32
  skips: [2]
  out_dim: 1
  last_activation: relu  # sigmoid # relu
  bound: 0.3
encoder:
  encoding: hashgrid
  input_dim: 3
  num_levels: 16
  level_dim: 4
  # base_resolution: 16 -> Too many memory
  base_resolution: 8
  log2_hashmap_size: 24
render:
  n_samples: 960  # 960  #576  # 768
  n_fine: 0
  perturb: True
  raw_noise_std: 0.
  netchunk: 409600
train:
  epoch: 1000
  n_batch: 1
  n_rays: 3500
  lrate: 0.00002 #0.00005
  lrate_gamma: 0.1
  lrate_step: 1000
  resume: True
log:
  i_eval: 100  # Epoch for evaluation
  i_save: 500  # Epoch for saving
loss:
  main: mse
  regularizer:
    weight: 0  # 0.0001  # 0.000001  # .00001
    scale: 4
  tv_2d: 0  # .000001
  tv_3d: 
    weight: 0  #.000001
    patch: 32
  ssim: .0000002
  patch: 50  #32